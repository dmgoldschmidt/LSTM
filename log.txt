Sat Oct 12 16:18:55 PDT 2019
Testing LSTMcell via test_LSTM.  I have one cell, and I'm getting "reasonable looking" output
with 2 time steps.  Next test:  generate 100 data points with random parameters, then perturb
the parameters and try to climb back.


Sun Oct 20 13:30:20 PDT 2019
Changed Gate.operator() to accept a RowVector<ColVector<double>> instead of three ColVectors {v,s,x}, and added dg_dW for the local parameter update.

Wed 21 Jul 2021 01:02:38 PM PDT
Restarting work on LSTM.  Changed access to ssh.

Wed 28 Jul 2021 02:25:25 PM PDT
There's been a lot of back and forth on how to do backprop.  Here's where
I'm at now:

1. Each weight matrix is of size (n_s, n_s+1) or (n_x,n_x+1).  Column 0
is the vector of biases.  The v,s, and x vectors ("signals") are all
augmented by a 0-component equal to 1 so they are of dimension n_s+1,n_s+1,
n_x +1 respectively, while the product is of dimention n_s or n_x. 

Note that the matrix vector products are only used to element-wise multiply
the signal components of index > 0, i.e. the weights just determine gating
values.

2. The LSTM class maintains an Array of LSTMcells. Each LSTMcell maintains
an Array of 4 Gates as per the diagram.  Each Gate has a forward_step and a
backward_step.  The forward step saves the v,s input vectors and the g
output vector.  These are all used by the backward step. 

Fri 30 Jul 2021 03:55:34 PM PDT
I now have preliminary code for Gate::f_step, Gate::b_step, LSTM_cell::
forward/backward step.

Thu 05 Aug 2021 04:59:43 PM PDT
Clean compile for LSTM.cc and test_LSTM.cc (one Cell only, no LSTM itself).

Sat 07 Aug 2021 03:45:54 PM PDT
Hmm, I'm running into a big problem with dE_dW (a matrix of matrices).  Compiles, but segfaults
with various bizarre results from gdb.  I'm thinking that there is some sort of bug in MemoryBlock
which is somehow stepping on dynamic memory.  Anyway, I'm going to redesign W and dE_dW to be big matrices
consisting the of concatenation of all 12 weight/bias matrices and their gradients.  Then in the gate
initialization code (which I'm going to change from constructor to reset) I will define six matrices
W_v, W_s, W_x and dE_dW_v, dE_dW_s, dE_dW_x as slices of the big matrix.  So the only matrices around
will be Matrix<double>.  Someday when I have time, I will try to figure out what's wrong with MemoryBlock. 
Note that W_v and W_s are n_s x (n_s+1) while W_x is n_s x (n_x+1). So W will be 4 x ( (3n_s x 2n_s+n_x + 3) )

Mon 09 Aug 2021 01:47:02 PM PDT
OK, I recoded LSTM.h,cc to use a packed parameter matrix W, and I now have a clean compile and a complete
execution of test_LSTM with one Cell.  No idea if the answer is correct, though.

Sat 28 Aug 2021 08:42:08 AM PDT
I've been lax about log entries, and there's been a fair amount of water under
the bridge since the last post.  As of now, I've started a new branch:
soft_max, with two significant changes: a) the data format is now 1-hot,
for categorical data, and b) the error computation is now soft_max to get
p = prob(goal), followed by error = -log(p), which is back-propped at every
cell. So in effect, n_x = n_s = #categories.  Right now, the code is very
murky due to all the soft copying of v,s,dE_dv,dE_ds.  I'm about to clean
this up.  I also want to eliminate augmented vectors, because it's too
bug-prone.  Instead, I will use two slices into W: bias_k and weights_k
for k = v,s,x in each Gate.  I should probably rename W -> P for parameters.

TO DO: check the dE_dW update for how bias is being handled.  This would
need to be done separately.

For continuous data, the soft_max output could be input e.g. into a
Gaussian mixture output distribution without much additional effort. And
we could re-estimate the Gaussian parameters a la HMM if we wanted to.
In this case, we could run the observations through the output model
to get "gammas" i.e. "soft 1-hot vectors" for input to the LSTM.  This
seems like it would be an interesting hybrid model.


