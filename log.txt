Sat Oct 12 16:18:55 PDT 2019
Testing LSTMcell via test_LSTM.  I have one cell, and I'm getting "reasonable looking" output
with 2 time steps.  Next test:  generate 100 data points with random parameters, then perturb
the parameters and try to climb back.


Sun Oct 20 13:30:20 PDT 2019
Changed Gate.operator() to accept a RowVector<ColVector<double>> instead of three ColVectors {v,s,x}, and added dg_dW for the local parameter update.

Wed 21 Jul 2021 01:02:38 PM PDT
Restarting work on LSTM.  Changed access to ssh.

Wed 28 Jul 2021 02:25:25 PM PDT
There's been a lot of back and forth on how to do backprop.  Here's where
I'm at now:

1. Each weight matrix is of size (n_s, n_s+1) or (n_x,n_x+1).  Column 0
is the vector of biases.  The v,s, and x vectors ("signals") are all
augmented by a 0-component equal to 1 so they are of dimension n_s+1,n_s+1,
n_x +1 respectively, while the product is of dimention n_s or n_x. 

Note that the matrix vector products are only used to element-wise multiply
the signal components of index > 0, i.e. the weights just determine gating
values.

2. The LSTM class maintains an Array of LSTMcells. Each LSTMcell maintains
an Array of 4 Gates as per the diagram.  Each Gate has a forward_step and a
backward_step.  The forward step saves the v,s input vectors and the g
output vector.  These are all used by the backward step. 

Fri 30 Jul 2021 03:55:34 PM PDT
I now have preliminary code for Gate::f_step, Gate::b_step, LSTM_cell::
forward/backward step.

Thu 05 Aug 2021 04:59:43 PM PDT
Clean compile for LSTM.cc and test_LSTM.cc (one Cell only, no LSTM itself).

Sat 07 Aug 2021 03:45:54 PM PDT
Hmm, I'm running into a big problem with dE_dW (a matrix of matrices).  Compiles, but segfaults
with various bizarre results from gdb.  I'm thinking that there is some sort of bug in MemoryBlock
which is somehow stepping on dynamic memory.  Anyway, I'm going to redesign W and dE_dW to be big matrices
consisting the of concatenation of all 12 weight/bias matrices and their gradients.  Then in the gate
initialization code (which I'm going to change from constructor to reset) I will define six matrices
W_v, W_s, W_x and dE_dW_v, dE_dW_s, dE_dW_x as slices of the big matrix.  So the only matrices around
will be Matrix<double>.  Someday when I have time, I will try to figure out what's wrong with MemoryBlock. 
Note that W_v and W_s are n_s x (n_s+1) while W_x is n_s x (n_x+1). So W will be 4 x ( (3n_s x 2n_s+n_x + 3) )

Mon 09 Aug 2021 01:47:02 PM PDT
OK, I recoded LSTM.h,cc to use a packed parameter matrix W, and I now have a clean compile and a complete
execution of test_LSTM with one Cell.  No idea if the answer is correct, though.

Mon 06 Sep 2021 02:46:31 PM PDT
OK, I'm back on the master branch.  The soft_max branch is still there, but it didn't really work any better
than what I was already doing.  I've been doing some online reading about LSTM, and am totally disgusted
with the poor quality of the articles.  As unfortunately seems typical of CS writing, almost every term is
either undefined, multiply defined, or at best poorly defined.  So far, the best bet is the developer docs
under "keras documentation".  After following a few links there, I discovered that a keras "unit" is just an
LSTM cell, but get this:  the parameter "nunits=xx" in one of the setup sequences has nothing to do with how many cells
are going to exist!!  Believe it or not, it actually refers to the dimension of the hidden state space!!!
Apparently, keras assigns one cell per timestep, and the no. of timesteps is inferred from the size of the data
matrix (you have to give it all the data at once).

I came across a "tutorial" at some point that provided python code for a toy LSTM problem using keras. And there was
a space for comments/questions, so I asked a few questions.  Turned out the author either a) completely misunderstood
or ignored the questions, and b) confidently asserted that "there are no cells in keras" which is of course absurd.

Apparently, the secret of LSTM is to have a high-dimensional hidden state space.  So far, I have no idea how high,
and in any case that will make the readout the same high dimension and therefore uninterpretable directly.  So
it looks like another "output layer" is needed to convert the readout to something from which an error and its
gradient can be computed.  So far, I have no documentation on this.  It is conveniently ignored in "LSTM fundamentals"
which I've followed to code the LSTM.  Presumably, this layer would be some set of standard squashed affine maps,
maybe one such map for every different training goal.  I will try a few more things on the master branch before
suspending work until I know more.

Fri 17 Sep 2021 01:55:25 PM PDT
Well, I tried a few more things:  n_s = 50 and various different values of alpha, but no joy.  I'm pretty sure there's
still some bugs, but I I just don't think I know enough about what's going on to debug the code intelligently.  So
I'm suspending the project for now.

